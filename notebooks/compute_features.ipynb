{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for preparing data\n",
    "\n",
    "This script downloads the data and extracts features for MegaDescriptor. We first load the required packages and specify where the data and extracted features will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "from wildlife_datasets import datasets\n",
    "from sides_matching import get_features, get_transform\n",
    "from sides_matching import amvrakikos, reunion_green, reunion_hawksbill, zakynthos\n",
    "\n",
    "root_data = '/data/wildlife_datasets/data'\n",
    "root_features = '../features'\n",
    "data = [\n",
    "    ('Amvrakikos', os.path.join(root_data, 'AmvrakikosTurtles'), amvrakikos),\n",
    "    ('ReunionGreen', os.path.join(root_data, 'ReunionTurtles'), reunion_green),\n",
    "    ('ReunionHawksbill', os.path.join(root_data, 'ReunionTurtles'), reunion_hawksbill),\n",
    "    ('Zakynthos', os.path.join(root_data, 'ZakynthosTurtles'), zakynthos),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we download the data. If an error appears, Kaggle is probably not setup. In such a case, either download the data manually or follow the link in the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_class in [datasets.AmvrakikosTurtles, datasets.ReunionTurtles, datasets.ZakynthosTurtles]:    \n",
    "    root = os.path.join(root_data, dataset_class.__name__)\n",
    "    dataset_class.get_data(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the features by the MegaDescriptor (large flavour) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from wildlife_tools.features import DeepFeatures, AlikedExtractor, SiftExtractor\n",
    "\n",
    "model_name = 'hf-hub:BVRA/MegaDescriptor-L-384'\n",
    "img_size = 384\n",
    "batch_size = 32\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "os.makedirs(root_features, exist_ok=True)\n",
    "for name, root, dataset_class in data:\n",
    "    for flip in [True, False]:\n",
    "        for grayscale in [True, False]:\n",
    "            transform = get_transform(flip=flip, grayscale=grayscale, img_size=img_size, normalize=True)\n",
    "            dataset = dataset_class(root, transform=transform, load_label=True)\n",
    "            # MegaDescriptor\n",
    "            file_name = os.path.join(root_features, f'MegaDescriptor_{name}_flip={flip}_grayscale={grayscale}.pickle')\n",
    "            if not os.path.exists(file_name):\n",
    "                model = timm.create_model(model_name, num_classes=0, pretrained=True)\n",
    "                extractor = DeepFeatures(model, batch_size=batch_size, device=device)\n",
    "                features = get_features(file_name, dataset, extractor)\n",
    "            # Aliked\n",
    "            file_name = os.path.join(root_features, f'Aliked_{name}_flip={flip}_grayscale={grayscale}_{img_size}.pickle')\n",
    "            if not os.path.exists(file_name):\n",
    "                extractor = AlikedExtractor(batch_size=batch_size, device=device)\n",
    "                features = get_features(file_name, dataset, extractor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
