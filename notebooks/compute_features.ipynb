{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for preparing data\n",
    "\n",
    "This script downloads the data and extracts features for MegaDescriptor. We first load the required packages and specify where the data and extracted features will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import torch\n",
    "from wildlife_datasets import datasets\n",
    "from sides_matching import WD, get_extractor, get_normalized_features, get_transform\n",
    "\n",
    "root_data = '../data'\n",
    "root_features = '../features'\n",
    "data = [\n",
    "    ('AmvrakikosTurtles', datasets.AmvrakikosTurtles),\n",
    "    ('ReunionTurtles', datasets.ReunionTurtles),\n",
    "    ('ZakynthosTurtles', datasets.ZakynthosTurtles),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we download the data. If an error appears, Kaggle is probably not setup. In such a case, either download the data manually or follow the link in the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset_class in data:\n",
    "    root = os.path.join(root_data, dataset_name)\n",
    "    dataset_class.get_data(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the features by the MegaDescriptor (large flavour) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hf-hub:BVRA/MegaDescriptor-L-384'\n",
    "img_size = 384\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "for dataset_name, dataset_class in data:\n",
    "    for grayscale in [True, False]:\n",
    "        transform = get_transform(flip=False, grayscale=grayscale, img_size=img_size, normalize=True)\n",
    "        root = os.path.join(root_data, dataset_name)\n",
    "        file_name = os.path.join(root_features, f'features_{dataset_name}_flip={False}_grayscale={grayscale}.npy')\n",
    "        if not os.path.exists(file_name):\n",
    "            d = dataset_class(root)\n",
    "            img_load = 'bbox' if 'bbox' in d.df else 'full'\n",
    "            dataset = WD(d.df, d.root, transform=transform, img_load=img_load)\n",
    "            extractor = get_extractor(model_name=model_name, batch_size=32, device=device)\n",
    "            features = get_normalized_features(file_name, dataset, extractor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
